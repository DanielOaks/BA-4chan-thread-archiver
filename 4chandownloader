#!/usr/bin/env python
# coding: utf-8

# Part of the JSON-based-chanarchiver by Lawrence Wu, 2012/04/04
# Originally from https://github.com/socketubs/4chandownloader
# Rewritten to save in seperate images folder, download plain HTML, modularization, comments, and code cleanup

#
# Initial release Nov. 5, 2009
# v6 release Jan. 20, 2009
# http://cal.freeshell.org
#
# Refactor, update and Python package
# by Socketubs (http://socketubs.net/)
# 09-08-12
#

import os
import time
import json
from docopt import docopt
import requests

import re
import errno

doc = """4chandownloader.py, uses 4chan API to download thread images and/or 
thumbnails, along with thread HTML, JSON, and a list of referenced external
links.

Usage:
  4chandownloader.py <url> <path> [--delay=<int>] [--nothumbs] [--thumbsonly]
  4chandownloader.py -h | --help
  4chandownloader.py -v | --version

Options:
  --nothumbs          Don't download thumbnails
  --thumbsonly        Download thumbnails, no images
  --delay=<int>       Delay between thread checks [default: 20]
  -h --help           Show help
  -v --version        Show version
"""

# Recursively create paths if they don't exist 
# replace with `os.makedirs(path,exist_ok=True)` in python3
def make_sure_path_exists(path):
    try:
        os.makedirs(path)
    except OSError as exception:
        if exception.errno != errno.EEXIST:
            raise

# Download any file using requests
def download_file(fname, dst_dir, file_url):
    # Destination of downloaded file
    file_dst = os.path.join(dst_dir, fname)
    
    # If the file doesn't exist, download it
    if not os.path.exists(file_dst):
	print('%s downloading...' % fname)
	i = requests.get(file_url)
	if i.status_code == 404:
	    print(' | Failed, try later (%s)' % file_url)
	else:
	    open(file_dst, 'w').write(i.content)
    else:
	print('%s already downloaded' % fname)

# File in place regex function originally scripted by steveha on StackOverflow: 
# http://stackoverflow.com/questions/1597649/replace-strings-in-files-by-python
# Notice: `\1` notation could be interpreted by python as `\x01`! Escape it with a second backslash: `\\1`  
def file_replace(fname, pat, s_after):
    # first, see if the pattern is even in the file.
    with open(fname) as f:
	if not any(re.search(pat, line) for line in f):
	    return # pattern does not occur in file so we are done.

    # pattern is in the file, so perform replace operation.
    with open(fname) as f:
	out_fname = fname + ".tmp"
	out = open(out_fname, "w")
	for line in f:
	    out.write(re.sub(pat, s_after, line))
	out.close()
	os.rename(out_fname, fname)

def main(args):
    # Copy data from arguments
    thread = args.get('<url>').split('/')[5]
    board  = args.get('<url>').split('/')[3]
    path   = args.get('<path>')
    nothumbs = args.get('--nothumbs', False)
    thumbsonly = args.get('--thumbsonly', False)
    delay  = args.get('--delay')

    # paths to 4chan URLs (with regex!)
    fourchan_api_url = 'https://api.4chan.org/%s/res/%s.json'
    fourchan_images_url = 'https://images.4chan.org/%s/src/%s'
    fourchan_thumbs_url = 'https://thumbs.4chan.org/%s/thumb/%s'
    fourchan_boards_url = 'http://boards.4chan.org/%s/res/%s'
    fourchan_images_regex = re.compile("http://images.4chan.org/\w+/src/")
    fourchan_thumbs_regex = re.compile("http://\d+.thumbs.4chan.org/\w+/thumb/")

    # The Ultimate URL Regex
    # http://stackoverflow.com/questions/520031/whats-the-cleanest-way-to-extract-urls-from-a-string-using-python
    linkregex = re.compile(r"""((?:[a-z][\w-]+:(?:/{1,3}|[a-z0-9%])|www\d{0,3}[.]|[a-z0-9.\-]+[.‌​][a-z]{2,4}/)(?:[^\s()<>]+|(([^\s()<>]+|(([^\s()<>]+)))*))+(?:(([^\s()<>]+|(‌​([^\s()<>]+)))*)|[^\s`!()[]{};:'".,<>?«»“”‘’]))""", re.DOTALL)

    # paths to file destination folders
    # path - folder/board/thread/
    # image path - folder/board/thread/img/<image-timestamp>.<ext>
    # thumbnails path - folder/board/thread/thumb/<image-timestamp>s.jpg
    image_dir_name = "img"
    thumb_dir_name = "thumb"
    dst_dir = os.path.join(path, board, thread)
    dst_images_dir = os.path.join(dst_dir, image_dir_name)
    dst_thumbs_dir = os.path.join(dst_dir, thumb_dir_name)

    # Begin file dump loop
    while 1:
	print(' :: Board: %s' % board)
	print(' :: Thread: %s' % thread)
	
	# Create paths if they don't exist
	# Don't create images folder if user only wants thumbnails
	# Create thumbs directory if thumbnails are needed
	make_sure_path_exists(dst_dir)
	if not thumbsonly:
	    make_sure_path_exists(dst_images_dir)
	if thumbsonly or (not nothumbs):
	    make_sure_path_exists(dst_thumbs_dir)
	
	# Grab thread JSON from 4chan API
	r = requests.get(fourchan_api_url % (board, thread))
	
	# File to store list of all external links quoted in comments (overwrite upon each loop iteration)
	linklist_dst = os.path.join(dst_dir, "external_links.txt")
	linklist_file = open(linklist_dst, "w")
	
	for post in r.json['posts']:
	    if post.get('filename', False):
		# If file is deleted, move on
		if post.get('filedeleted', False):
		    continue
		
		# Download images, if not only downloading thumbnails
		if not thumbsonly:
		    image_name = '%s%s' % (post['tim'], post['ext'])
		    image_url  = fourchan_images_url % (board, image_name)
		    download_file(image_name, dst_images_dir, image_url)

		# Download thumbnails by default, but not if user doesn't want it
		if thumbsonly or (not nothumbs):
		    thumb_name = '%ss.jpg' % post['tim']
		    thumb_url = fourchan_thumbs_url % (board, thumb_name)
		    download_file(thumb_name, dst_thumbs_dir, thumb_url)
		
	    # Go to the next comment if link not found
	    if not linkregex.search(post['com']):
		continue
	    else:
		# We need to get rid of all <wbr> tags before parsing
		cleaned_com = re.sub(r'\<wbr\>', '', post['com'])
		linklist = re.findall(linkregex, cleaned_com)
		for item in linklist:
		    print("Found link to external site, saving in %s:\n%s\n" % (linklist_dst, item[0]))
		    linklist_file.write(item[0])	# re.findall creates tuple
		    linklist_file.write('\n')	# subdivide with newlines

	# Close linklist file after loop
	linklist_file.close()

	# Dumps thread in raw HTML format to `<thread-id>.html`
	html_filename = "%s.html" % thread
	html_url = fourchan_boards_url % (board, thread)
	download_file(html_filename, dst_dir, html_url)
	
	# Convert all links in HTML dump to use locally downloaded files
	html_path = os.path.join(dst_dir, html_filename)
	file_replace(html_path, '"//', '"http://')
	file_replace(html_path, fourchan_images_regex, image_dir_name + "/")
	file_replace(html_path, fourchan_thumbs_regex, thumb_dir_name + "/")
	
	# (future) Download a local copy of all CSS files, and convert HTML links to use them (we need to use beautifulsoup to get links...)
	#file_replace(html_path, "http://static.4chan.org/css/(\w+.\d+).css", "css/\\1.css")
	
	# Dumps thread in JSON format to `<thread-id>.json` file
	json_filename = "%s.json" % thread
	json_path = os.path.join(dst_dir, json_filename)
	json.dump(r.json, open(json_path, 'w'))

	# Wait to execute code again
	print("Waiting %s seconds before retrying" % delay)
	time.sleep(int(delay))

if __name__ == '__main__':
  args = docopt(doc, version=0.3)
  main(args)
